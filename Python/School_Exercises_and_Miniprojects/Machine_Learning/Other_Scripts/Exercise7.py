# -*- coding: utf-8 -*-
"""
Created on Wed Oct 24 09:40:38 2018

@author: Laura

Develop an MLP for the MNIST database by using the dimension-reduced data from 
your work on Exercises 2 and 3. You can download the LDA projected data here.
Further, you can use 10-, 20- and 30-dimensional data generated by PCA and 
compare their performance (at the same time, try various MLP architectures).
"""
import matplotlib.pyplot as plt
import numpy as np
import scipy.io as io
from sklearn.neural_network import MLPClassifier
from sklearn.decomposition import PCA
from baysean_classfier import baysean_cls_gauss as bcg

# =============================================================================
# Data
# =============================================================================
""" LDA DATA """
data_lda = io.loadmat("mnist_lda.mat")

X = data_lda['train_data']       # Vector of size 60000 x 9
y = data_lda['train_class']      # Vector of size 60000 x 1
Y = data_lda['train_class_01']   # Matrix of size 60000 x 10    

X_test = data_lda['test_data']   # Vector of size 10000 x 9
y_test = data_lda['test_class']  # Vector of size 10000 x 1

""" PCA DATA """
data-pca = io.loadmat("mnist_all.mat")
k = 10

label_data = {i: len(data_pca['train'+str(i)]) for i in range(k)}           # label_dict
vstack_data = np.vstack((data_pca['train'+str(i)] for i in range(k)))       # v-stack data (AVD)
array_data = np.array([data_pca['train'+str(i)] for i in range(k)])         # Array data (AD)

label = np.hstack((np.ones(len(data['train'+str(i)]))*i for i in range(k))) # h-stack labels

# Test data
TD = np.vstack((data['test'+str(j)] for j in range(k)))

# List of labels
l = np.hstack((np.ones(len(data['test'+str(i)]))*i for i in range(k)))

# =============================================================================
# MLP model - LDA
# =============================================================================
clf = MLPClassifier(solver='lbfgs', alpha=1e-5, 
                    hidden_layer_sizes=(25,), random_state=1)
clf.fit(X, y)                         

y_predict = clf.predict(X_test)

l_t = y_test
# =============================================================================
# MLP model - PCA
# =============================================================================
#""" Data management """
#k = 10
## label_dict
#N_data = {i: len(data['train'+str(i)]) for i in range(k)}
#
## concatonated if needed (It is)
#AVD = np.vstack((data['train'+str(i)] for i in range(k)))
#
## Arrayed DATA
#AD = np.array([data['train'+str(i)] for i in range(k)])
#
## concatonated if needed (It is)
#label = np.hstack((np.ones(len(data['train'+str(i)]))*i for i in range(k)))
#
## Test data
#TD = np.vstack((data['test'+str(j)] for j in range(k)))
#
## List of labels
#l = np.hstack((np.ones(len(data['test'+str(i)]))*i for i in range(k)))
#
#pca_10 = PCA(n_components=10)
##pca_20 = PCA(n_components=20)
##pca_30 = PCA(n_components=30)
#
#x_p1 = pca_10.fit(AVD).transform(AVD)
##x_p2 = pca_20.fit(AVD).transform(AVD)
##x_p3 = pca_30.fit(AVD).transform(AVD)
#
#fit_pca1 = pca_10.fit(AVD).transform(TD)
##pre_pca2 = pca_10.fit(AVD).transform(TD)
#
#print("Equivelant PCA dimension 10")
#model = bcg(x_p1, label, N_data)
#model.print_accuracy(pre_pca1, l)
#print("\n")

# =============================================================================
# Comparison
# =============================================================================
def accuracy(cls, test_labels):
        """
        Performs classification and compares with the class labels assuming
        integer lables.
        """
        N = len(test_labels)

        # Calculate total correct as precentage
        total_correct = 100*(N - np.count_nonzero(cls - test_labels.T))/N

        # Calculate precentag correct for each class
        lab = np.unique(test_labels)
        cls_correct = {}
        for label in lab:
            idx = np.where(test_labels == label)[0]
            N_cls = len(idx)
            cls_correct[label] = 100*(N_cls - np.count_nonzero(label -
                                                               cls[idx]))/N_cls

        print("Accuracy for:")
        print("All classes is %.2f%%" % total_correct)
        for label in lab:
            print("Class %d is %.2f%%" % (label, cls_correct[label]))
        return(total_correct, cls_correct)


print("\n")
print("LDA with a dimensional reduction to 9 - predict with MLP")
tc, cc = accuracy(y_predict, l_t)
print("\n")


# =============================================================================
# Information about MLPClassifier
# =============================================================================
"""
MLPClassifier(activation='relu',           # f(x) = max(0,x)), default = relu
              alpha=1e-05,                 # Regularization term, default = 0.0001
              batch_size='auto',           # Minibatch min(200, n_samples), default = auto
              beta_1=0.9, beta_2=0.999,    # Exponential decay rate for ADAM
              early_stopping=False,        # If validation score isn't improved 
              epsilon=1e-08,               # Numerical stability in ADAM
              hidden_layer_sizes=(5, 2),   # Number of neuron in NN
              learning_rate='constant',    # Constant learning rate given by ‘learning_rate_init'
              learning_rate_init=0.001,    # Controls the step-size in updating the weights, default = 0.001
              max_iter=200,                # Maximum number of iterations (Epochs)
              #momentum=0.9,               # Momentum for gradient descent update, used when solver='sgd'
              n_iter_no_change=10,         # Maximum number of epochs to not meet 'tol' improvement
              #nesterovs_momentum=True,    # Only used when solver=’sgd’ and momentum > 0
              #power_t=0.5,                # The exponent for inverse scaling learning rate, used for solver='sgd'
              random_state=1,              # Random number generator  
              shuffle=True,                # Shuffle samples in each iteration
              solver='adam',               # Solver for weight optimization
              tol=0.0001,                  # Tolerance for the optimization, default = 0.0001
              #validation_fraction=0.1,    # The proportion of training data to set aside as validation set for early stopping, used when 'early_stopping= True' 
              verbose=False,               # Whether to print progress messages to stdout.
              warm_start=False)            # Erase the previous solution
"""

